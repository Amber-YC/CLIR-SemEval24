Problem Statement:
In this task, we consider a Zero-Shot Learning problem, wherein we are provided with monolingual sentence pairs from source languages with semantic relatedness scoring and ranking, as well as unscored sentence pairs from other low-resource (even distant) target languages.
The objective during test time is to predict the scores and rank monolingual sentence pairs in unlabeled target languages, leveraging source language semantic similarity information. Based on cross-lingual word representations within a dense vector space, we aim  to implement a crosslingual semantic relatedness scoring and ranking system.




Model: Transformer + Adapter
Encoder (mBERT): Given the absence of parallel word or sentence pairs for cross-lingual alignment, we use mBERT, a Pretrained Massively Multilingual Transformer (MMT), as our encoder to acquire cross-lingual word representations in a dense vector space. Utilizing this, we derive sentence representations for each sentence.
Adapter: Inspired by parameter-efficient approaches to crosslingual- transfer, instead of straightforward encoder fine-tuning, our key idea is to design a language-neutral Adapter structure (potentially we will also try other parameter-efficient approaches like LoRA or SFTMs) implemented atop the Transformer, enhancing the efficiency of the training process. This structure will acquire knowledge of the downstream task of semantic textual similarity ranking.
During the training process, sentence pairs from the source language are initially passed through the Transformer (mBERT) layer. Subsequently, Scoring Adapters are trained using source language data, with all Transformer parameters frozen.
Subsequently, we calculate the cosine similarity of the output from the Adapter Structure for two sentences, generating semantic relatedness scores. The sentence pairs are then ranked in descending order based on these scores.
Figure 1 illustrates the workflow of our model.
  

(Figure 1)






Literature (For This Proposal):
Zero-Shot Learning for Cross Lingual NLP Tasks:


1. Guanhua Chen, Shuming Ma, Yun Chen, Li Dong, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2021. Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 15–26, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
2. Zihan Liu, Jamin Shin, Yan Xu, Genta Indra Winata, Peng Xu, Andrea Madotto, and Pascale Fung. 2019. Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1297–1303, Hong Kong, China. Association for Computational Linguistics.
3. Anne Lauscher, Vinit Ravishankar, Ivan Vulić, and Goran Glavaš. 2020. From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483–4499, Online. Association for Computational Linguistics.
Parameter-efficient Transfer:
1. Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers for Text Ranking: BERT and Beyond. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorials, pages 1–4, Online. Association for Computational Linguistics.
2. Robert Litschko, Ivan Vulić, and Goran Glavaš. 2022. Parameter-Efficient Neural Reranking for Cross-Lingual and Multilingual Retrieval. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1071–1082, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.
3. Yang, E., Nair, S., Lawrie, D.C., Mayfield, J., & Oard, D.W. (2022). Parameter-efficient Zero-shot Transfer for Cross-Language Dense Retrieval with Adapters. ArXiv, abs/2212.10448.
4. Jonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings EMNLP, pages 7654–7673.